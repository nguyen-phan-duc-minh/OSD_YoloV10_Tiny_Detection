{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a427eb7",
   "metadata": {},
   "source": [
    "# Drone Object Detection Challenge - Complete Pipeline\n",
    "\n",
    "Notebook này chứa toàn bộ code để thực hiện Drone Object Detection Challenge. Bao gồm:\n",
    "\n",
    "1. **Setup và cài đặt dependencies**\n",
    "2. **Khám phá dữ liệu**\n",
    "3. **Training model**\n",
    "4. **Evaluation với ST-IoU metric**\n",
    "5. **Export TensorRT cho Jetson NX**\n",
    "6. **Tạo submission file**\n",
    "\n",
    "## Mục tiêu\n",
    "- Detect object trong video drone dựa trên reference images\n",
    "- Optimize cho NVIDIA Jetson Xavier NX (≤50M parameters)\n",
    "- Đạt ≥25 FPS real-time inference\n",
    "- Maximize ST-IoU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07dfca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/macos/Downloads/Zalo/drone_detection\n",
      "Python version: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thêm project root vào Python path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bd002",
   "metadata": {},
   "source": [
    "## Cài đặt Dependencies\n",
    "\n",
    "Trước khi chạy, hãy đảm bảo đã cài đặt tất cả các dependencies cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7dfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt dependencies cho OSD-YOLOv10 - Production\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\"ultralytics\", \"timm\", \"albumentations\", \"tensorboard\", \"opencv-python\", \"einops\"]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a13ec7",
   "metadata": {},
   "source": [
    "## Khám phá dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfa23f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data paths\n",
    "data_dir = project_root.parent / \"data\"\n",
    "train_dir = data_dir / \"train\"\n",
    "test_dir = data_dir / \"public_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73f7b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "annotations_path = train_dir / \"annotations\" / \"annotations.json\"\n",
    "with open(annotations_path, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e6ecc",
   "metadata": {},
   "source": [
    "## Load Model và Setup Training\n",
    "\n",
    "Bây giờ chúng ta sẽ load model và setup training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models và setup\n",
    "from models.ultra_light_detector import UltraLightDroneDetector, create_ultra_light_model\n",
    "from models.light_osd_yolov10 import LightOSDYOLOv10\n",
    "from utils.dataset import DroneDataset, collate_fn\n",
    "from utils.evaluation import compute_st_iou, evaluate_video\n",
    "\n",
    "# Load config from Python file\n",
    "from configs.default import config\n",
    "\n",
    "# Update data paths\n",
    "config['data']['train_dir'] = str(data_dir)\n",
    "config['data']['test_dir'] = str(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "173c13ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Parameters: 12,689,322\n",
      "Memory (FP16): 24.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Create Production Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create Ultra-Light model for production\n",
    "model = create_ultra_light_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Parameters: {model.count_parameters():,}\")\n",
    "print(f\"Memory (FP16): {(model.count_parameters() * 2) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470e1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset created: 1400 samples\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "import utils.dataset\n",
    "from utils.dataset import DroneDataset, collate_fn\n",
    "\n",
    "train_dataset = DroneDataset(\n",
    "    data_dir=str(data_dir),\n",
    "    split=\"train\",\n",
    "    frame_sampling_rate=5,\n",
    "    max_frames_per_video=100,\n",
    "    augmentation=True,\n",
    "    image_size=(640, 640)\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328aaee",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "Bây giờ chúng ta sẽ implement training loop để train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7564e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = batch['images'].to(device)\n",
    "        object_images = batch['object_images'].to(device)\n",
    "        \n",
    "        targets = []\n",
    "        for i in range(len(batch['video_ids'])):\n",
    "            if len(batch['bboxes'][i]) > 0:\n",
    "                targets.append({\n",
    "                    'boxes': batch['bboxes'][i].to(device),\n",
    "                    'labels': batch['labels'][i].to(device)\n",
    "                })\n",
    "            else:\n",
    "                targets.append({\n",
    "                    'boxes': torch.empty(0, 4, device=device),\n",
    "                    'labels': torch.empty(0, dtype=torch.long, device=device)\n",
    "                })\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, object_images, targets)\n",
    "        \n",
    "        if 'losses' in output:\n",
    "            loss = output['losses']['total_loss']\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "# Setup optimizer và scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=config['training']['learning_rate'], weight_decay=config['training']['weight_decay'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=config['training']['epochs'])\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1ada72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/175 [03:25<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m checkpoints_dir.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     epoch_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     scheduler.step()\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device)\u001b[39m\n\u001b[32m      8\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m      9\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobject_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mobject_images\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1172\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1165\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1171\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1174\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:47\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     reduction.dump(prep_data, fp)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     49\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:57\u001b[39m, in \u001b[36mQueue.__getstate__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sys.platform != \u001b[33m'\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     55\u001b[39m         register_after_fork(\u001b[38;5;28mself\u001b[39m, Queue._after_fork)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     58\u001b[39m     context.assert_spawning(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._ignore_epipe, \u001b[38;5;28mself\u001b[39m._maxsize, \u001b[38;5;28mself\u001b[39m._reader, \u001b[38;5;28mself\u001b[39m._writer,\n\u001b[32m     60\u001b[39m             \u001b[38;5;28mself\u001b[39m._rlock, \u001b[38;5;28mself\u001b[39m._wlock, \u001b[38;5;28mself\u001b[39m._sem, \u001b[38;5;28mself\u001b[39m._opid)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = config['training']['epochs']\n",
    "best_loss = float('inf')\n",
    "\n",
    "checkpoints_dir = project_root / \"checkpoints\"\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch(model, train_dataloader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_model_path = checkpoints_dir / \"best_model.pth\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'config': config,\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': epoch_loss\n",
    "        }\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f} - Best: {best_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29613970",
   "metadata": {},
   "source": [
    "## Evaluation với ST-IoU Metric\n",
    "\n",
    "Bây giờ chúng ta sẽ evaluate model sử dụng Spatio-Temporal IoU metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation với ST-IoU\n",
    "def evaluate_model_on_dataset(model, dataset, max_samples=500):\n",
    "    model.eval()\n",
    "    st_ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(max_samples, len(dataset))), desc=\"Evaluating\"):\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            images = sample['image'].unsqueeze(0).to(device)\n",
    "            object_images = sample['object_images'].unsqueeze(0).to(device)\n",
    "            \n",
    "            output = model(images, object_images)\n",
    "            predictions = output['predictions'][0]\n",
    "            \n",
    "            # Convert predictions\n",
    "            pred_list = []\n",
    "            for box, score, sim in zip(predictions['boxes'], predictions['scores'], predictions['similarities']):\n",
    "                if score > 0.5:\n",
    "                    pred_list.append({\n",
    "                        'frame': sample['frame_id'],\n",
    "                        'bbox': box.cpu().numpy().tolist(),\n",
    "                        'confidence': score.cpu().item(),\n",
    "                        'similarity': sim.cpu().item()\n",
    "                    })\n",
    "            \n",
    "            # Convert ground truth\n",
    "            gt_list = []\n",
    "            for box in sample['bboxes']:\n",
    "                gt_list.append({\n",
    "                    'frame': sample['frame_id'],\n",
    "                    'bbox': [box[0].item(), box[1].item(), box[2].item(), box[3].item()]\n",
    "                })\n",
    "            \n",
    "            # Compute ST-IoU\n",
    "            if len(pred_list) > 0 and len(gt_list) > 0:\n",
    "                st_iou = compute_st_iou(pred_list, gt_list)\n",
    "            else:\n",
    "                st_iou = 0.0\n",
    "            \n",
    "            st_ious.append(st_iou)\n",
    "    \n",
    "    return st_ious\n",
    "\n",
    "# Load best model và evaluate\n",
    "best_model_path = checkpoints_dir / \"best_model.pth\"\n",
    "if best_model_path.exists():\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "st_ious = evaluate_model_on_dataset(model, train_dataset)\n",
    "avg_st_iou = np.mean(st_ious)\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Average ST-IoU: {avg_st_iou:.4f}\")\n",
    "print(f\"Samples evaluated: {len(st_ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b8f20",
   "metadata": {},
   "source": [
    "## Save Model và Checkpoints\n",
    "\n",
    "Lưu model đã train để sử dụng sau này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model\n",
    "final_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'model_parameters': model.count_parameters(),\n",
    "    'avg_st_iou': avg_st_iou\n",
    "}\n",
    "\n",
    "final_model_path = checkpoints_dir / \"final_model.pth\"\n",
    "torch.save(final_checkpoint, final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"Average ST-IoU: {avg_st_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb12ed",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Tạo file submission cho test set theo format yêu cầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Submission\n",
    "def generate_submission(model, test_dir, output_path, confidence_threshold=0.3):\n",
    "    samples_dir = test_dir / \"samples\"\n",
    "    video_dirs = [d for d in samples_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    submission = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for video_dir in tqdm(video_dirs, desc=\"Processing test videos\"):\n",
    "            video_id = video_dir.name\n",
    "            \n",
    "            # Load reference images\n",
    "            object_images_dir = video_dir / \"object_images\"\n",
    "            ref_images = []\n",
    "            \n",
    "            for img_file in sorted(object_images_dir.glob(\"*.jpg\")):\n",
    "                img = cv2.imread(str(img_file))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "                ref_images.append(img)\n",
    "            \n",
    "            if len(ref_images) == 0:\n",
    "                submission.append({\"video_id\": video_id, \"detections\": []})\n",
    "                continue\n",
    "            \n",
    "            # Pad to 3 reference images\n",
    "            while len(ref_images) < 3:\n",
    "                ref_images.append(ref_images[-1] if ref_images else torch.zeros(3, 224, 224))\n",
    "            ref_images = ref_images[:3]\n",
    "            \n",
    "            ref_images_tensor = torch.stack(ref_images).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Process video\n",
    "            video_file = video_dir / \"drone_video.mp4\"\n",
    "            if not video_file.exists():\n",
    "                submission.append({\"video_id\": video_id, \"detections\": []})\n",
    "                continue\n",
    "            \n",
    "            cap = cv2.VideoCapture(str(video_file))\n",
    "            frame_detections = []\n",
    "            frame_idx = 0\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_resized = cv2.resize(frame_rgb, (640, 640))\n",
    "                frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
    "                frame_tensor = torch.from_numpy(frame_normalized).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                \n",
    "                output = model(frame_tensor, ref_images_tensor)\n",
    "                predictions = output['predictions'][0]\n",
    "                \n",
    "                for box, score, sim in zip(predictions['boxes'], predictions['scores'], predictions['similarities']):\n",
    "                    if score.item() > confidence_threshold:\n",
    "                        h, w = frame.shape[:2]\n",
    "                        x1, y1, x2, y2 = box.cpu().numpy()\n",
    "                        \n",
    "                        x1 = int(x1 * w / 640)\n",
    "                        y1 = int(y1 * h / 640)\n",
    "                        x2 = int(x2 * w / 640)\n",
    "                        y2 = int(y2 * h / 640)\n",
    "                        \n",
    "                        frame_detections.append({\n",
    "                            \"frame\": frame_idx,\n",
    "                            \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2,\n",
    "                            \"confidence\": float(score.item()),\n",
    "                            \"similarity\": float(sim.item())\n",
    "                        })\n",
    "                \n",
    "                frame_idx += 1\n",
    "            \n",
    "            cap.release()\n",
    "            submission.append({\"video_id\": video_id, \"detections\": frame_detections})\n",
    "    \n",
    "    # Save submission\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Generate submission\n",
    "submission_dir = project_root / \"submissions\"\n",
    "submission_path = submission_dir / \"final_submission.json\"\n",
    "\n",
    "submission = generate_submission(model, test_dir, submission_path)\n",
    "total_detections = sum(len(v['detections']) for v in submission)\n",
    "\n",
    "print(f\"Submission generated!\")\n",
    "print(f\"Total videos: {len(submission)}\")\n",
    "print(f\"Total detections: {total_detections}\")\n",
    "print(f\"Saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb822",
   "metadata": {},
   "source": [
    "## TensorRT Export cho Jetson NX\n",
    "\n",
    "Export model sang TensorRT để deploy trên Jetson Xavier NX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Model for Deployment\n",
    "def export_model_for_deployment(model, export_dir):\n",
    "    export_dir = Path(export_dir)\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test inputs\n",
    "    dummy_image = torch.randn(1, 3, 640, 640).to(device)\n",
    "    dummy_ref_images = torch.randn(1, 3, 3, 224, 224).to(device)\n",
    "    \n",
    "    # Export ONNX\n",
    "    onnx_path = export_dir / \"drone_detector.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model, (dummy_image, dummy_ref_images), str(onnx_path),\n",
    "        export_params=True, opset_version=11, do_constant_folding=True,\n",
    "        input_names=['image', 'reference_images'], output_names=['predictions'],\n",
    "        dynamic_axes={'image': {0: 'batch_size'}, 'reference_images': {0: 'batch_size'}}\n",
    "    )\n",
    "    \n",
    "    # Export TorchScript\n",
    "    traced_path = export_dir / \"drone_detector_traced.pt\"\n",
    "    traced_model = torch.jit.trace(model, (dummy_image, dummy_ref_images))\n",
    "    traced_model.save(str(traced_path))\n",
    "    \n",
    "    # Save complete model\n",
    "    complete_model_path = export_dir / \"drone_detector_complete.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'model_parameters': model.count_parameters()\n",
    "    }, complete_model_path)\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "# Export model\n",
    "export_dir = project_root / \"exports\"\n",
    "export_results = export_model_for_deployment(model, export_dir)\n",
    "\n",
    "print(f\"Model exported successfully!\")\n",
    "print(f\"Files saved to: {export_results}\")\n",
    "print(f\"Ready for deployment on Jetson Xavier NX!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
